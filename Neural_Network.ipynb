{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our neural network by subclassing nn.Module, and initialize the neural network \n",
    "# layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(nn.Linear(28*28, 512), \n",
    "                                               nn.ReLU(), \n",
    "                                               nn.Linear(512, 512),\n",
    "                                               nn.ReLU(), \n",
    "                                               nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create an instance of NeuralNetwork, and move it to the device, and print its structure.\n",
    "model = NeuralNetwork().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the model, we pass it the input data. This executes the model’s \n",
    "# forward, along with some background operations. Do not call model.forward() directly!\n",
    "\n",
    "# Calling the model on the input returns a 2-dimensional tensor with dim=0 \n",
    "# corresponding to each output of 10 raw predicted values for each class, \n",
    "# and dim=1 corresponding to the individual values of each output. We get \n",
    "# the prediction probabilities by passing it through an instance of the nn.Softmax module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0147,  0.0983,  0.0975,  0.0458,  0.0370,  0.0234, -0.0896, -0.0331,\n",
       "          0.0093, -0.0481]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probab = nn.Softmax(1)(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0998, 0.1085, 0.1084, 0.1029, 0.1020, 0.1006, 0.0899, 0.0951, 0.0992,\n",
       "         0.0937]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probab.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted class: {pred_probab.argmax(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s break down the layers in the FashionMNIST model. \n",
    "# To illustrate it, we will take a sample minibatch of 3 \n",
    "# images of size 28x28 and see what happens to it as we pass it through the network.\n",
    "\n",
    "input_image = torch.rand(3, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Flatten: We initialize the nn.Flatten layer to convert each 2D 28x28 image \n",
    "# into a contiguous array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained).\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4006, 0.2832, 0.3711,  ..., 0.1623, 0.2624, 0.7526],\n",
       "        [0.3472, 0.2779, 0.1446,  ..., 0.3127, 0.1642, 0.9204],\n",
       "        [0.9449, 0.7630, 0.7195,  ..., 0.0982, 0.4809, 0.5212]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Linear: The linear layer is a module that applies a linear transformation on the input using its stored weights and biases.\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2044,  0.1716, -0.2079,  0.2493, -0.0959, -0.3697, -0.2178,  0.4640,\n",
       "         -0.2125,  0.3842,  0.5408, -0.0531, -0.1709,  0.1705,  0.5511,  0.3166,\n",
       "         -0.3704,  0.6866,  0.3204,  0.4556],\n",
       "        [ 0.0283,  0.3484,  0.1538,  0.2839, -0.4455,  0.0571, -0.4884,  0.5781,\n",
       "          0.2348,  0.0044,  0.8396,  0.0811, -0.1592, -0.1072,  0.1215,  0.0431,\n",
       "         -0.5144,  0.2720,  0.3181,  0.5103],\n",
       "        [-0.2061,  0.2797, -0.2497,  0.4385, -0.4037, -0.1647, -0.1745,  0.3845,\n",
       "         -0.2738,  0.2703,  0.7450,  0.0116, -0.3886, -0.3296,  0.3000,  0.0020,\n",
       "         -0.2971,  0.3299,  0.3991,  0.2904]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.ReLU:\n",
    "# Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n",
    "\n",
    "# In this model, we use nn.ReLU between our linear layers, but there’s other activations to introduce non-linearity in your model.\n",
    "\n",
    "hidden1 = nn.ReLU()(hidden1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1716, 0.0000, 0.2493, 0.0000, 0.0000, 0.0000, 0.4640, 0.0000,\n",
       "         0.3842, 0.5408, 0.0000, 0.0000, 0.1705, 0.5511, 0.3166, 0.0000, 0.6866,\n",
       "         0.3204, 0.4556],\n",
       "        [0.0283, 0.3484, 0.1538, 0.2839, 0.0000, 0.0571, 0.0000, 0.5781, 0.2348,\n",
       "         0.0044, 0.8396, 0.0811, 0.0000, 0.0000, 0.1215, 0.0431, 0.0000, 0.2720,\n",
       "         0.3181, 0.5103],\n",
       "        [0.0000, 0.2797, 0.0000, 0.4385, 0.0000, 0.0000, 0.0000, 0.3845, 0.0000,\n",
       "         0.2703, 0.7450, 0.0116, 0.0000, 0.0000, 0.3000, 0.0020, 0.0000, 0.3299,\n",
       "         0.3991, 0.2904]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential: nn.Sequential is an ordered container of modules. \n",
    "# The data is passed through all the modules in the same order as defined. \n",
    "# You can use sequential containers to put together a quick network like seq_modules.\n",
    "\n",
    "seq_modules = nn.Sequential(flatten, layer1, nn.ReLU(), nn.Linear(20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(3, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = seq_modules(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1716, -0.1598, -0.0588, -0.0139,  0.0119,  0.0295, -0.0887,  0.0615,\n",
       "          0.2024, -0.4259],\n",
       "        [ 0.1770, -0.1340, -0.1727,  0.0135,  0.0113,  0.0030, -0.0843,  0.1033,\n",
       "          0.1761, -0.4408],\n",
       "        [ 0.1998, -0.2518, -0.0946,  0.0586,  0.0856,  0.1007, -0.0135,  0.1168,\n",
       "          0.2666, -0.4593]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Softmax: The last linear layer of the neural network returns logits - raw values in \n",
    "# [-infty, infty] - which are passed to the nn.Softmax module. The logits are\n",
    "#  scaled to values [0, 1] representing the model’s predicted probabilities \n",
    "# for each class. dim parameter indicates the dimension along which the values must sum to 1.\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1203, 0.0864, 0.0956, 0.1000, 0.1026, 0.1044, 0.0927, 0.1078, 0.1241,\n",
       "         0.0662],\n",
       "        [0.1218, 0.0892, 0.0858, 0.1034, 0.1032, 0.1023, 0.0938, 0.1131, 0.1217,\n",
       "         0.0657],\n",
       "        [0.1196, 0.0761, 0.0891, 0.1038, 0.1067, 0.1083, 0.0966, 0.1101, 0.1278,\n",
       "         0.0619]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Structure: Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=20, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=20, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model's Structure: {seq_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure: <bound method Module.named_parameters of Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=20, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=20, out_features=10, bias=True)\n",
      ")>\n",
      "Layer: 1.weight | Size: torch.Size([20, 784]) | Values : tensor([[-0.0023,  0.0238, -0.0104,  ..., -0.0167, -0.0289,  0.0037],\n",
      "        [ 0.0250, -0.0104,  0.0095,  ...,  0.0038,  0.0012,  0.0181]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: 1.bias | Size: torch.Size([20]) | Values : tensor([0.0179, 0.0346], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: 3.weight | Size: torch.Size([10, 20]) | Values : tensor([[-1.1528e-03,  1.2637e-01, -1.4101e-01, -1.5631e-03,  6.6131e-02,\n",
      "          8.9990e-02,  9.4388e-02, -2.7156e-03, -1.8415e-02,  1.0227e-01,\n",
      "          1.2607e-01, -1.8378e-01,  5.2246e-02,  1.8180e-01, -5.7845e-02,\n",
      "         -9.2231e-02, -1.1765e-01,  2.1407e-01, -1.7229e-01,  5.8319e-02],\n",
      "        [-2.2179e-01,  1.6645e-01,  6.2918e-02, -8.9028e-02,  3.0388e-03,\n",
      "         -4.5578e-02, -4.9959e-02, -1.0707e-01,  7.0507e-03,  4.8413e-02,\n",
      "         -9.3015e-02,  1.3417e-02,  1.6831e-01, -3.0600e-02,  1.1971e-01,\n",
      "          7.6070e-05,  5.2384e-02,  1.3023e-01, -1.4818e-01, -9.8212e-02]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: 3.bias | Size: torch.Size([10]) | Values : tensor([-0.0091, -0.1198], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model Structure: {seq_modules.named_parameters}\")\n",
    "for name, param in seq_modules.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
