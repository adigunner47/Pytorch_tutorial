{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Differentiation with torch.autograd\n",
    "\n",
    "# When training neural networks, the most frequently used algorithm is back propagation. \n",
    "# In this algorithm, parameters (model weights) are adjusted according to the gradient \n",
    "# of the loss function with respect to the given parameter.\n",
    "\n",
    "# To compute those gradients, PyTorch has a built-in differentiation engine called \n",
    "# torch.autograd. It supports automatic computation of gradient for any computational graph.\n",
    "\n",
    "# Consider the simplest one-layer neural network, with input x, parameters w and b, and \n",
    "# some loss function. It can be defined in PyTorch in the following manner:\n",
    "\n",
    "x = torch.ones(5)\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "y = torch.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.matmul(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3836, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000001612D616230>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000001612BE16D40>\n"
     ]
    }
   ],
   "source": [
    "# In this network, w and b are parameters, which we need to optimize. \n",
    "# Thus, we need to be able to compute the gradients of loss function \n",
    "# with respect to those variables. In order to do that, we set the \n",
    "# requires_grad property of those tensors.\n",
    "\n",
    "# A function that we apply to tensors to construct computational graph \n",
    "# is in fact an object of class Function. This object knows how to \n",
    "# compute the function in the forward direction, and also how to compute \n",
    "# its derivative during the backward propagation step. A reference to \n",
    "# the backward propagation function is stored in grad_fn property of a \n",
    "# tensor. You can find more information of Function in the documentation.\n",
    "\n",
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Gradients:\n",
    "# To optimize weights of parameters in the neural network, we need to compute the derivatives \n",
    "# of our loss function with respect to parameters, namely, we need dloss/dw and dloss/db\n",
    "# under some fixed values of x and y. To compute those derivatives, we call loss.backward(), \n",
    "# and then retrieve the values from w.grad and b.grad:\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2567, 0.1565, 0.2903],\n",
      "        [0.2567, 0.1565, 0.2903],\n",
      "        [0.2567, 0.1565, 0.2903],\n",
      "        [0.2567, 0.1565, 0.2903],\n",
      "        [0.2567, 0.1565, 0.2903]])\n",
      "tensor([0.2567, 0.1565, 0.2903])\n"
     ]
    }
   ],
   "source": [
    "#We can only obtain the grad properties for the leaf nodes of \n",
    "# the computational graph, which have requires_grad property \n",
    "# set to True. For all other nodes in our graph, gradients will not be available.\n",
    "\n",
    "# We can only perform gradient calculations using backward once on \n",
    "# a given graph, for performance reasons. If we need to do several \n",
    "# backward calls on the same graph, we need to pass retain_graph=True \n",
    "# to the backward call.\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Disabling Gradient Tracking:\n",
    "# By default, all tensors with requires_grad=True are tracking their \n",
    "# computational history and support gradient computation. However, there \n",
    "# are some cases when we do not need to do that, for example, when we \n",
    "# have trained the model and just want to apply it to some input data, \n",
    "# i.e. we only want to do forward computations through the network. We \n",
    "# can stop tracking computations by surrounding our computation code with torch.no_grad() block:\n",
    "with torch.no_grad():\n",
    "    z_nograd = torch.matmul(x, w)+b\n",
    "print(z_nograd.requires_grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z tensor: tensor([ 1.2092, -0.1219,  1.9080], grad_fn=<AddBackward0>)\n",
      "Z gradient: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Z tensor: {z}\")\n",
    "print(f\"Z gradient: {z.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z tensor: tensor([ 1.2092, -0.1219,  1.9080])\n",
      "Z gradient: False\n"
     ]
    }
   ],
   "source": [
    "# Another way to achieve the same result is to use the detach() method on the tensor:\n",
    "z_no_grad = z.detach()\n",
    "print(f\"Z tensor: {z_no_grad}\")\n",
    "print(f\"Z gradient: {z_no_grad.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #There are reasons you might want to disable gradient tracking:\n",
    "# To mark some parameters in your neural network as frozen parameters.\n",
    "\n",
    "# To speed up computations when you are only doing forward pass, because \n",
    "# computations on tensors that do not track gradients would be more efficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
